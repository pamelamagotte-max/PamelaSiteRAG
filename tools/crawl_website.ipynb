{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsL29WWpUijI1YepEQunWx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pamelamagotte-max/PamelaSiteRAG/blob/main/tools/crawl_website.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "zU8hRoxIw9Eu",
        "outputId": "55bf584e-84a2-432e-c03d-0bdb14d1156b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "f-string expression part cannot include a backslash (ipython-input-943657472.py, line 48)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-943657472.py\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    f.write(f'title: \"{title.replace(\\'\"\\', \"\\'\")}\"\\n')\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string expression part cannot include a backslash\n"
          ]
        }
      ],
      "source": [
        "# 1) Installer les libs\n",
        "!pip -q install requests beautifulsoup4 markdownify trafilatura python-slugify tldextract tqdm\n",
        "\n",
        "# 2) Le script du crawler (copi√© tel quel ici)\n",
        "START_URL = \"https://pamelamagotte.fr/\"   # <- change si besoin\n",
        "MAX_PAGES = 400                            # <- mets la limite que tu veux\n",
        "\n",
        "import os, re, time, hashlib, requests\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from markdownify import markdownify as md\n",
        "from slugify import slugify\n",
        "from tqdm import tqdm\n",
        "\n",
        "OUT_DIR = \"corpus\"\n",
        "\n",
        "def same_host(url, base): return urlparse(url).netloc == urlparse(base).netloc\n",
        "BAD_EXT = (\".pdf\",\".jpg\",\".jpeg\",\".png\",\".gif\",\".webp\",\".svg\",\".zip\",\".mp3\",\".mp4\",\".avi\",\".mov\",\".wmv\",\".ico\")\n",
        "HEADERS = {\"User-Agent\": \"PamelaBot/1.0 (+https://assistant-pamela.streamlit.app)\"}\n",
        "\n",
        "def fetch(u):\n",
        "    try:\n",
        "        r = requests.get(u, headers=HEADERS, timeout=20)\n",
        "        if r.status_code == 200 and \"text/html\" in r.headers.get(\"Content-Type\",\"\"):\n",
        "            return r.text\n",
        "    except: pass\n",
        "    return None\n",
        "\n",
        "def clean_html(html, base_url):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for tag in soup([\"script\",\"style\",\"noscript\",\"iframe\"]): tag.decompose()\n",
        "    for tag in soup.select(\"nav, footer, aside, form\"): tag.decompose()\n",
        "    title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
        "    main = soup.select_one(\"article\") or soup.select_one(\"main\") or soup.body\n",
        "    if not main: return title, \"\"\n",
        "    for a in main.find_all(\"a\", href=True): a[\"href\"] = urljoin(base_url, a[\"href\"])\n",
        "    text_md = md(str(main), strip=[\"img\"])\n",
        "    text_md = re.sub(r\"\\n{3,}\", \"\\n\\n\", text_md).strip()\n",
        "    return title, text_md\n",
        "\n",
        "def write_md(title, url, body_md):\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    if not title: title = urlparse(url).path.strip(\"/\").replace(\"/\",\" \") or \"page\"\n",
        "    slug = slugify(title)[:80] or hashlib.sha1(url.encode()).hexdigest()[:10]\n",
        "    path = os.path.join(OUT_DIR, f\"{slug}.md\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"---\\n\")\n",
        "        f.write(f'title: \"{title.replace(\\'\"\\', \"\\'\")}\"\\n')\n",
        "        f.write(f'url: \"{url}\"\\nsource: \"website\"\\n---\\n\\n')\n",
        "        f.write(body_md.strip() + \"\\n\")\n",
        "    return path\n",
        "\n",
        "def crawl():\n",
        "    seen, queue = set(), [START_URL]\n",
        "    count = 0\n",
        "    while queue and count < MAX_PAGES:\n",
        "        u = queue.pop(0)\n",
        "        if u in seen: continue\n",
        "        seen.add(u)\n",
        "        if u.lower().endswith(BAD_EXT): continue\n",
        "        if not same_host(u, START_URL): continue\n",
        "        html = fetch(u)\n",
        "        if not html: continue\n",
        "        title, body = clean_html(html, u)\n",
        "        if len(body) < 200: continue\n",
        "        write_md(title, u, body); count += 1\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            v = urljoin(u, a[\"href\"])\n",
        "            if same_host(v, START_URL) and v not in seen:\n",
        "                if not v.lower().endswith(BAD_EXT):\n",
        "                    queue.append(v)\n",
        "        time.sleep(0.2)\n",
        "    print(f\"OK: {count} fichiers dans /content/{OUT_DIR}\")\n",
        "\n",
        "crawl()\n"
      ]
    }
  ]
}